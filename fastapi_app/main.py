from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
import sys

# --- ãƒ‘ã‚¹è¨­å®š ---
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œå ´æ‰€ã«é–¢ã‚ã‚‰ãšã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
# ã“ã‚Œã«ã‚ˆã‚Šã€'scraper'ã‚„'rag_handler'ã®ã‚ˆã†ãªãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®‰å®šã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã‚‹
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# --- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# rag_handlerã¯èµ·å‹•æ™‚ã«åˆæœŸåŒ–ã•ã‚Œã€ãƒ™ã‚¯ãƒˆãƒ«DBãªã©ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€
from rag_handler import rag_handler_instance
# scraperã¯/scrapeã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã®ã¿ä½¿ç”¨ã™ã‚‹
from scraper import crawl_website

# --- FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ– ---
app = FastAPI(
    title="RAG Chatbot API",
    description="ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨RAGï¼ˆRetrieval-Augmented Generationï¼‰ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆAPIã§ã™ã€‚`/docs`ã‹ã‚‰å¯¾è©±çš„ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚",
    version="1.0.0"
)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾© (Pydantic) ---
# APIãŒå—ã‘å–ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®å‹ã‚’å®šç¾©

class ChatQuery(BaseModel):
    """ /chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ """
    question: str

class ScrapeRequest(BaseModel):
    """ /scrape ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ """
    urls: list[str]

# --- APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾© ---

@app.get("/", summary="ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯")
async def root():
    """
    APIã‚µãƒ¼ãƒãƒ¼ãŒæ­£å¸¸ã«èµ·å‹•ã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚
    """
    return {"status": "ok", "message": "API is running. Please head to /docs to test the endpoints."}

@app.post("/chat", summary="ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«è³ªå•ã‚’é€ä¿¡")
async def chat_endpoint(query: ChatQuery):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚’å—ã‘å–ã‚Šã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’åŸºã«ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã‚’è¿”ã—ã¾ã™ã€‚
    """
    try:
        # rag_handlerã®ãƒ¡ã‚½ãƒƒãƒ‰åã‚’ `query` ã‹ã‚‰ `ask` ã«ä¿®æ­£
        response = rag_handler_instance.ask(query.question)
        
        # rag_handlerå†…ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†
        if "error" in response:
            raise HTTPException(status_code=500, detail=response["error"])
        
        # å›ç­”ã¨ã‚½ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å«ã‚€å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ã‚ˆã†ã«ä¿®æ­£
        return response
    except Exception as e:
        # äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {str(e)}")

@app.post("/scrape", summary="Webã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰")
async def scrape_endpoint(request: ScrapeRequest):
    """
    æŒ‡å®šã•ã‚ŒãŸURLãƒªã‚¹ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã¾ãŸã¯æ›´æ–°ã—ã¾ã™ã€‚
    ã“ã®å‡¦ç†ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚å®Œäº†å¾Œã€RAGã‚·ã‚¹ãƒ†ãƒ ã¯æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§å†åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚
    """
    if not request.urls:
        raise HTTPException(status_code=400, detail="URL list cannot be empty.")
    
    try:
        print("ğŸš€ Starting web scraping...")
        crawl_website(request.urls) # scraper.pyã®é–¢æ•°ã‚’å®Ÿè¡Œ
        
        print("ğŸ”„ Re-initializing RAG handler with new data...")
        # æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§RAGãƒãƒ³ãƒ‰ãƒ©ã‚’å†åˆæœŸåŒ–
        rag_handler_instance._initialize()
        
        return {"message": "Scraping and knowledge base update completed successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to scrape or re-initialize: {str(e)}")
