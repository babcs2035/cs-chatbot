import os
import jq
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import JSONLoader
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.chains import RetrievalQA
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder


# --- å®šæ•°å®šç¾© ---
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "mxbai-embed-large")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3:latest")
RERANKER_MODEL = "BAAI/bge-reranker-base"
CHROMA_DB_PATH = "./chroma_db"
DATA_PATH = "./data/scraped_data.json"
CHUNK_SIZE = 512
CHUNK_OVERLAP = 128

prompt_template = """
### æŒ‡ç¤º
ã‚ãªãŸã¯ã€æä¾›ã•ã‚ŒãŸã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã‚’æ·±ãç†è§£ã—ã€è³ªå•ã«å¯¾ã—ã¦çš„ç¢ºã«å›ç­”ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³å®ˆã—ã¦ãã ã•ã„ã€‚

1.  å›ç­”ã¯ã€å¿…ãšã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã«åŸºã¥ã„ã¦ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
2.  æƒ…å ±ã‚’ãŸã æŠœãå‡ºã™ã®ã§ã¯ãªãã€**çµ±åˆãƒ»è¦ç´„ã—ã€è‡ªç„¶ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« ã§å›ç­”ã‚’ä½œæˆ**ã—ã¦ãã ã•ã„ã€‚
3.  è³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«å­˜åœ¨ã—ãªã„å ´åˆã¯ã€ã€Œãã®æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨æ˜ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚è‡ªèº«ã®çŸ¥è­˜ã§è£œå®Œã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
4.  å›ç­”ã¯æ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚

### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
{context}

### è³ªå•
{question}

### å›ç­”
"""


class RAGHandler:
    def __init__(self):
        self.vector_store = None
        self.qa_chain = None
        self._initialize()

    def _initialize(self):
        embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

        if os.path.exists(CHROMA_DB_PATH) and os.listdir(CHROMA_DB_PATH):
            print("âœ… Loading existing vector store...")
            self.vector_store = Chroma(
                persist_directory=CHROMA_DB_PATH, embedding_function=embeddings
            )
        else:
            print(f"â³ Creating new vector store using '{EMBEDDING_MODEL}'...")
            if not os.path.exists(DATA_PATH):
                raise FileNotFoundError(
                    f"'{DATA_PATH}' not found. Please run scraper.py first."
                )

            loader = JSONLoader(
                file_path=DATA_PATH,
                jq_schema=".[] | .content",
                text_content=True,
                json_lines=False,
            )
            documents = loader.load()

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
            )
            docs = text_splitter.split_documents(documents)

            self.vector_store = Chroma.from_documents(
                documents=docs, embedding=embeddings, persist_directory=CHROMA_DB_PATH
            )
            print(f"âœ… Vector store created with {len(docs)} document chunks.")

        llm = ChatOllama(model=LLM_MODEL, temperature=0.2, top_k=40)

        QA_CHAIN_PROMPT = PromptTemplate(
            input_variables=["context", "question"],
            template=prompt_template,
        )

        base_retriever = self.vector_store.as_retriever(
            search_type="mmr", search_kwargs={"k": 10}
        )

        model = HuggingFaceCrossEncoder(model_name=RERANKER_MODEL)
        reranker = CrossEncoderReranker(model=model, top_n=3)

        compression_retriever = ContextualCompressionRetriever(
            base_compressor=reranker, base_retriever=base_retriever
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=compression_retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
        )
        print("ğŸš€ RAG Handler is ready with fine-tuned parameters.")

    def ask(self, query: str):
        if self.qa_chain is None:
            return {"error": "QA chain is not initialized."}

        try:
            print(f"ğŸ” Query: {query}")
            response = self.qa_chain.invoke({"query": query})
            sources = [
                doc.metadata.get("source", "Unknown")
                for doc in response.get("source_documents", [])
            ]

            print(f"âœ… Answer: {response.get('result')}")
            return {
                "answer": response.get("result"),
                "source_documents": list(set(sources)),
            }
        except Exception as e:
            return {"error": f"Failed to get an answer from the QA chain: {str(e)}"}


rag_handler_instance = RAGHandler()
