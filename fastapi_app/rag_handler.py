"""
é«˜åº¦ãªRAG (Retrieval-Augmented Generation) ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å‡¦ç†ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ï¼
ä»¥ä¸‹ã®å…ˆé€²çš„ãªæŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã¦ï¼Œå›ç­”ã®ç²¾åº¦ã¨åŠ¹ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ï¼
1. ã‚¯ã‚¨ãƒªæ‹¡å¼µ (Query Expansion): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’å¤šæ§˜åŒ–ã—ï¼Œæ¤œç´¢ã®ç¶²ç¾…æ€§ã‚’å‘ä¸Šï¼
2. æ–‡è„ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ¤œç´¢ (Sentence-Window Retrieval): æ¤œç´¢ç²¾åº¦ã¨æ–‡è„ˆã®è±Šã‹ã•ã‚’ä¸¡ç«‹ï¼
3. RAG-Fusion (RRF): è¤‡æ•°ã®æ¤œç´¢çµæœã‚’çµ±åˆã—ï¼Œæœ€ã‚‚ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±ã‚’æŠ½å‡ºï¼
4. æŠ½å‡ºâ†’çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: LLMãŒãƒã‚¤ã‚ºã‚’é™¤å»ã—ã¦ã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ï¼Œç²¾åº¦ã‚’å‘ä¸Šï¼
5. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿæ§‹: åŸ‹ã‚è¾¼ã¿ã¨LLMã®å¿œç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ï¼Œå‡¦ç†ã‚’é«˜é€ŸåŒ–ï¼
"""

import os
import jq
import logging
import hashlib
import asyncio
import json
from typing import Any, Dict, List, Generator
from tqdm import tqdm
from langchain_core.documents import Document
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# LangChainã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.globals import set_llm_cache

# LangChainã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
from langchain_community.cache import InMemoryCache
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.document_loaders import JSONLoader
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# ChromaDBã®è¨­å®š
from chromadb.config import Settings

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s",
)
logger = logging.getLogger(__name__)


# --- ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã§ä½¿ç”¨ã™ã‚‹å®šæ•° ---
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "mxbai-embed-large")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3:latest")
RERANKER_MODEL = "BAAI/bge-reranker-base"
CHROMA_DB_PATH = "./db"
DATA_PATH = "./data/scraped_data.json"
CACHE_PATH = "./cache"
PARENT_DOCS_PATH = "./data/parent_docs.json"
# æ–‡è„ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ¤œç´¢ã§ä½¿ç”¨ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
PARENT_CHUNK_SIZE = 1024  # LLMã«æ¸¡ã™ï¼Œæ–‡è„ˆãŒè±Šå¯Œãªè¦ªãƒãƒ£ãƒ³ã‚¯
CHILD_CHUNK_SIZE = 256  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ä½¿ã†ï¼Œç²¾åº¦ã®é«˜ã„å­ãƒãƒ£ãƒ³ã‚¯
CHUNK_OVERLAP = 64
# DBæ§‹ç¯‰æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚º
BATCH_SIZE = 128

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«LLMã®ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–
logger.info("ğŸ§  Activating in-memory LLM cache...")
set_llm_cache(InMemoryCache())


# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå®šç¾© ---

# æ–¹é‡1ã€Œã‚¯ã‚¨ãƒªæ‹¡å¼µã€ã§ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
query_expansion_template = """
### æŒ‡ç¤º
ã‚ãªãŸã¯ï¼Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åˆ†æã—ï¼Œãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®å¤šæ§˜ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ï¼
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ã‚’åŸºã«ï¼Œç•°ãªã‚‹è¦–ç‚¹ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”¨ã„ãŸæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’3ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ï¼

### å³æ ¼ãªãƒ«ãƒ¼ãƒ«
- **å‡ºåŠ›ã¯"queries"ã¨ã„ã†ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿**ã¨ã—ï¼Œå‰å¾Œã«èª¬æ˜ã‚„æŒ¨æ‹¶ãªã©ã®ä½™è¨ˆãªãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼
- å¿…ãš3ã¤ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ï¼Œã™ã¹ã¦æ—¥æœ¬èªã§æ›¸ã„ã¦ãã ã•ã„ï¼
- å‡ºåŠ›å½¢å¼ã¯å¿…ãš `{{"queries": ["ã‚¯ã‚¨ãƒª1", "ã‚¯ã‚¨ãƒª2", "ã‚¯ã‚¨ãƒª3"]}}` ã®ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼

### å…ƒã®è³ªå•
{question}
"""

# RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã€ŒæŠ½å‡ºã€ã‚¹ãƒ†ãƒƒãƒ—ã§ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸåºƒç¯„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ï¼Œè³ªå•ã«ç›´æ¥é–¢é€£ã™ã‚‹æƒ…å ±ã®ã¿ã‚’LLMã«æŠœãå‡ºã•ã›ã‚‹
extraction_template = """
### æŒ‡ç¤º
ã‚ãªãŸã¯ï¼Œæä¾›ã•ã‚ŒãŸã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã®ä¸­ã‹ã‚‰ï¼Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œè³ªå•ã€ã«å›ç­”ã™ã‚‹ãŸã‚ã«å¿…è¦ä¸å¯æ¬ ãªæƒ…å ±ã®ã¿ã‚’æ­£ç¢ºã«æŠœãå‡ºã™AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ï¼
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ï¼Œé–¢é€£ã™ã‚‹æ–‡ç« ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼

### ãƒ«ãƒ¼ãƒ«
- ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã®å„æ–‡ç« ã‚’è©•ä¾¡ã—ï¼Œã€Œè³ªå•ã€ã¸ã®å›ç­”ã«ç›´æ¥é–¢é€£ã™ã‚‹æ–‡ç« ã ã‘ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼
- æŠ½å‡ºã—ãŸæ–‡ç« ã¯ï¼Œå…ƒã®æ–‡ç« ã‹ã‚‰ä¸€åˆ‡å¤‰æ›´ã‚’åŠ ãˆãšï¼Œãã®ã¾ã¾å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
- è¤‡æ•°ã®é–¢é€£æ–‡ç« ãŒã‚ã‚‹å ´åˆã¯ï¼Œãã‚Œãã‚Œã‚’æ”¹è¡Œã§åŒºåˆ‡ã£ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
- é–¢é€£ã™ã‚‹æƒ…å ±ãŒå…¨ããªã„å ´åˆã¯ï¼Œä½•ã‚‚å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ï¼

### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
{context}

### è³ªå•
{question}

### æŠ½å‡ºçµæœ
"""

# æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆï¼ˆçµ±åˆã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã™ã‚‹ãŸã‚ã®ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
answer_generation_template = """
### æŒ‡ç¤º
ã‚ãªãŸã¯æ±äº¬å¤§å­¦ã®æƒ…å ±ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹å­¦ç”Ÿã‚„æ•™è·å“¡ã‹ã‚‰ã®è³ªå•ã«å›ç­”ã™ã‚‹ï¼Œéå¸¸ã«å„ªç§€ã§è¦ªåˆ‡ãªã‚µãƒãƒ¼ãƒˆæ‹…å½“è€…ã§ã™ï¼
æä¾›ã•ã‚ŒãŸã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã‚’æ³¨æ„æ·±ãèª­ã¿è¾¼ã¿ï¼Œä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼

### ãƒ«ãƒ¼ãƒ«
1.  **è¨€èª**: å›ç­”ã¯ã™ã¹ã¦æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ï¼
2.  **å½¹å‰²ã®éµå®ˆ**: å¿…ãšã‚µãƒãƒ¼ãƒˆæ‹…å½“è€…ã¨ã—ã¦ï¼Œä¸å¯§ã§åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰é£ã„ã‚’ã—ã¦ãã ã•ã„ï¼
3.  **æƒ…å ±ã®å³å®ˆ**: å›ç­”ã¯ï¼Œ**æä¾›ã•ã‚ŒãŸã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿**ã‚’æ ¹æ‹ ã¨ã—ã¦ãã ã•ã„ï¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨˜è¼‰ã®ãªã„æƒ…å ±ã‚„ï¼Œã‚ãªãŸè‡ªèº«ã®çŸ¥è­˜ã‚’æ±ºã—ã¦è£œã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ï¼
4.  **æ®µéšçš„ãªèª¬æ˜**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ï¼Œå…·ä½“çš„ãªæ‰‹é †ã‚’æ®µéšçš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ï¼
5.  **çµ±åˆã¨è¦ç´„**: è¤‡æ•°ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚ç…§ã—ï¼Œãã‚Œã‚‰ã‚’è‡ªç„¶ãªæ–‡ç« ã«çµ±åˆãƒ»è¦ç´„ã—ã¦å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼æƒ…å ±ã®æ–­ç‰‡ã‚’ã‚³ãƒ”ãƒ¼ãƒšãƒ¼ã‚¹ãƒˆã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ï¼
6.  **æƒ…å ±ãŒãªã„å ´åˆ**: è³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ï¼Œæ›–æ˜§ãªå›ç­”ã¯ã›ãšï¼Œã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒï¼Œã”è³ªå•ã«é–¢ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ã€ã¨æ˜ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ï¼

### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
{context}

### è³ªå•
{question}

### å›ç­”
"""


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ ---


def metadata_func(record: dict, metadata: dict) -> dict:
    """JSONLoaderã§æ–‡æ›¸ã‚’èª­ã¿è¾¼ã‚€éš›ã«ï¼ŒURLã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä»˜ä¸ã™ã‚‹ï¼"""
    metadata["source"] = record.get("url")
    return metadata


def batch_generator(
    data: List[Any], batch_size: int
) -> Generator[List[Any], None, None]:
    """ãƒªã‚¹ãƒˆã‚’æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆ†å‰²ã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼"""
    for i in range(0, len(data), batch_size):
        yield data[i : i + batch_size]


def create_sha256_encoder(namespace: str):
    """åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã®ã‚­ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆã™ã‚‹ï¼"""

    def sha256_encoder(payload: str) -> str:
        # ãƒ¢ãƒ‡ãƒ«åï¼ˆnamespaceï¼‰ã‚’ã‚­ãƒ¼ã«å«ã‚ã‚‹ã“ã¨ã§ï¼Œç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¡çªã—ãªã„ã‚ˆã†ã«ã™ã‚‹
        namespaced_payload = f"{namespace}:{payload}".encode("utf-8")
        return hashlib.sha256(namespaced_payload).hexdigest()

    return sha256_encoder


class RAGHandler:
    """é«˜åº¦ãªRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç®¡ç†ãƒ»å®Ÿè¡Œã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ã‚¯ãƒ©ã‚¹ï¼"""

    def __init__(self):
        """ãƒãƒ³ãƒ‰ãƒ©ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹ï¼"""
        self.vector_store = None
        self.parent_docstore = {}
        self.llm = None
        self.json_llm = None
        self.reranker = None
        self.query_expansion_chain = None
        self.extraction_chain = None
        self.final_answer_chain = None
        logger.info("ğŸš¦ Initializing Advanced RAGHandler...")
        self._initialize()

    def _initialize(self):
        """RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã™ã¹ã¦åˆæœŸåŒ–ã™ã‚‹ï¼"""
        # --- 1. ãƒ¢ãƒ‡ãƒ«ã¨åŸºæœ¬ãƒ„ãƒ¼ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
        os.makedirs(CACHE_PATH, exist_ok=True)
        fs_store = LocalFileStore(CACHE_PATH)

        logger.info("ğŸ”Œ Connecting to Ollama at %s", OLLAMA_BASE_URL)
        ollama_embedder = OllamaEmbeddings(
            model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL
        )
        key_encoder = create_sha256_encoder(namespace=EMBEDDING_MODEL)
        cached_embedder = CacheBackedEmbeddings.from_bytes_store(
            ollama_embedder, fs_store, key_encoder=key_encoder
        )

        self.llm = ChatOllama(
            model=LLM_MODEL, temperature=0.2, top_k=40, base_url=OLLAMA_BASE_URL
        )
        self.json_llm = ChatOllama(
            model=LLM_MODEL, format="json", temperature=0, base_url=OLLAMA_BASE_URL
        )

        logger.info("ğŸ” Initializing Re-ranker: %s", RERANKER_MODEL)
        self.reranker = HuggingFaceCrossEncoder(model_name=RERANKER_MODEL)

        # --- 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿ï¼Œã¾ãŸã¯æ–°è¦ä½œæˆ ---
        chroma_settings = Settings(anonymized_telemetry=False)
        if os.path.exists(CHROMA_DB_PATH) and os.path.exists(PARENT_DOCS_PATH):
            logger.info("ğŸ’¾ Loading existing vector store and parent docstore...")
            self.vector_store = Chroma(
                persist_directory=CHROMA_DB_PATH,
                embedding_function=cached_embedder,
                client_settings=chroma_settings,
            )
            with open(PARENT_DOCS_PATH, "r", encoding="utf-8") as f:
                parent_docs_data = json.load(f)
                self.parent_docstore = {
                    int(k): Document(**v) for k, v in parent_docs_data.items()
                }
            logger.info("âœ… Vector store and parent docstore loaded successfully.")
        else:
            self._create_sentence_window_db(cached_embedder, chroma_settings)

        # --- 3. LangChain Expression Language (LCEL) ã‚’ç”¨ã„ã¦å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆãƒã‚§ãƒ¼ãƒ³ï¼‰ã‚’å®šç¾© ---
        logger.info("ğŸ”— Initializing LCEL chains...")

        # è³ªå•ã‹ã‚‰è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹ãƒã‚§ãƒ¼ãƒ³
        query_prompt = PromptTemplate(
            template=query_expansion_template, input_variables=["question"]
        )
        self.query_expansion_chain = query_prompt | self.json_llm | JsonOutputParser()

        # æ¤œç´¢ã—ãŸæ–‡è„ˆã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ï¼ˆé–¢é€£éƒ¨åˆ†ã®ã¿æŠ½å‡ºï¼‰ã™ã‚‹ãƒã‚§ãƒ¼ãƒ³
        extraction_prompt = PromptTemplate(
            template=extraction_template, input_variables=["context", "question"]
        )
        self.extraction_chain = (
            {
                "context": lambda x: "\n---\n".join(
                    doc.page_content for doc in x["documents"]
                ),
                "question": lambda x: x["question"],
            }
            | extraction_prompt
            | self.llm
            | StrOutputParser()
        )

        # æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ãƒã‚§ãƒ¼ãƒ³
        answer_prompt = PromptTemplate(
            template=answer_generation_template, input_variables=["context", "question"]
        )
        self.final_answer_chain = (
            {
                "context": lambda x: x["cleaned_context"],
                "question": lambda x: x["question"],
            }
            | answer_prompt
            | self.llm
            | StrOutputParser()
        )
        logger.info("ğŸš€ Advanced RAG Handler is ready.")

    def _create_sentence_window_db(self, embedder, settings):
        """æ–‡è„ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æˆ¦ç•¥ã«åŸºã¥ãï¼Œãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨è¦ªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒˆã‚¢ã‚’æ–°è¦ä½œæˆã™ã‚‹ï¼"""
        logger.warning(
            "ğŸ•³ï¸ Vector store not found. Creating a new one with Sentence-Window strategy..."
        )
        if not os.path.exists(DATA_PATH):
            raise FileNotFoundError(f"'{DATA_PATH}' not found.")

        documents = JSONLoader(
            file_path=DATA_PATH,
            jq_schema=".[]",
            content_key="content",
            json_lines=False,
            metadata_func=metadata_func,
        ).load()
        logger.info("ğŸ“„ Loaded %d raw documents.", len(documents))

        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=PARENT_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
        )
        child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHILD_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
        )

        parent_documents = parent_splitter.split_documents(documents)
        self.parent_docstore = {i: doc for i, doc in enumerate(parent_documents)}
        logger.info("ğŸ“‘ Split into %d parent chunks.", len(parent_documents))

        child_documents = []
        for i, p_doc in enumerate(parent_documents):
            _sub_docs = child_splitter.split_documents([p_doc])
            for _doc in _sub_docs:
                _doc.metadata["parent_id"] = i  # å­ãƒãƒ£ãƒ³ã‚¯ã«è¦ªIDã‚’ç´ä»˜ã‘ã‚‹
            child_documents.extend(_sub_docs)
        logger.info("ğŸ“‘ Created %d child chunks for retrieval.", len(child_documents))

        logger.info(
            "â³ Creating vector store. This may take a while (first time only)..."
        )
        self.vector_store = Chroma(
            persist_directory=CHROMA_DB_PATH,
            embedding_function=embedder,
            client_settings=settings,
        )
        doc_batches = batch_generator(child_documents, BATCH_SIZE)
        for batch in tqdm(
            doc_batches,
            desc="ğŸ“¦ Embedding and adding child chunks",
            total=len(child_documents) // BATCH_SIZE
            + (1 if len(child_documents) % BATCH_SIZE > 0 else 0),
        ):
            self.vector_store.add_documents(batch)

        logger.info("ğŸ’¾ Saving parent docstore to %s", PARENT_DOCS_PATH)
        parent_docs_data = {k: v.dict() for k, v in self.parent_docstore.items()}
        with open(PARENT_DOCS_PATH, "w", encoding="utf-8") as f:
            json.dump(parent_docs_data, f, ensure_ascii=False, indent=4)

        logger.info("âœ… Vector store and parent docstore created.")

    async def ask(self, query: str) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ï¼Œé«˜åº¦ãªRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ï¼"""
        if not self.vector_store:
            return {"error": "Vector store not initialized."}

        try:
            # --- ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¯ã‚¨ãƒªæ‹¡å¼µ ---
            logger.info("ğŸ§  Step 1: Expanding query...")
            if self.query_expansion_chain is None:
                raise RuntimeError("Query expansion chain is not initialized.")
            response_dict = await self.query_expansion_chain.ainvoke(
                {"question": query}
            )
            expanded_queries = response_dict.get("queries", [])
            all_queries = [query] + expanded_queries
            logger.info(f"âš¡ Expanded queries: {all_queries}")

            # --- ã‚¹ãƒ†ãƒƒãƒ—2: ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ ---
            logger.info("ğŸ” Step 2: Retrieving documents in parallel...")
            retriever = self.vector_store.as_retriever(search_kwargs={"k": 15})
            tasks = [retriever.ainvoke(q) for q in all_queries]
            retrieved_results = await asyncio.gather(*tasks)

            # --- ã‚¹ãƒ†ãƒƒãƒ—3: RAG-Fusion (RRF) ---
            logger.info("âœ¨ Step 3: Fusing results with RRF...")
            fused_docs = self._rag_fusion(retrieved_results)
            logger.info(f"ğŸ—³ï¸ Fused to {len(fused_docs)} unique documents.")

            # --- ã‚¹ãƒ†ãƒƒãƒ—4: å†ãƒ©ãƒ³ã‚¯ä»˜ã‘ ---
            logger.info("ğŸ§ Step 4: Re-ranking top documents...")
            rerank_pairs = [(query, doc.page_content) for doc in fused_docs[:20]]
            if self.reranker is None:
                logger.warning(
                    "âš ï¸ Reranker is not initialized. Skipping reranking step."
                )
                final_child_docs = fused_docs[:4]
            else:
                scores = self.reranker.score(rerank_pairs)
                scored_docs = sorted(
                    zip(fused_docs, scores), key=lambda x: x[1], reverse=True
                )
                final_child_docs = [doc for doc, score in scored_docs[:4]]

            # --- ã‚¹ãƒ†ãƒƒãƒ—5: æ–‡è„ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å–å¾— ---
            logger.info("ğŸ“– Step 5: Fetching full context from parent docstore...")
            parent_ids = {doc.metadata["parent_id"] for doc in final_child_docs}
            final_context_docs = [
                self.parent_docstore.get(pid)
                for pid in parent_ids
                if self.parent_docstore.get(pid) is not None
            ]
            logger.info(
                f"ğŸ“š Using {len(final_context_docs)} parent documents for initial context."
            )

            # --- ã‚¹ãƒ†ãƒƒãƒ—6: æ–‡è„ˆã®æŠ½å‡ºï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰---
            logger.info("âœ‚ï¸ Step 6: Extracting relevant sentences from context...")
            if self.extraction_chain is None:
                logger.warning(
                    "âš ï¸ Extraction chain is not initialized. Skipping extraction step."
                )
                cleaned_context = "\n---\n".join(
                    doc.page_content for doc in final_context_docs if doc is not None
                )
            else:
                cleaned_context = await self.extraction_chain.ainvoke(
                    {"documents": final_context_docs, "question": query}
                )
                if not cleaned_context or cleaned_context.isspace():
                    logger.warning(
                        "âš ï¸ No relevant information found after extraction. Falling back to original context."
                    )
                    cleaned_context = "\n---\n".join(
                        doc.page_content
                        for doc in final_context_docs
                        if doc is not None
                    )  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                else:
                    logger.info("âœ… Context cleaned successfully.")

            # --- ã‚¹ãƒ†ãƒƒãƒ—7: æœ€çµ‚å›ç­”ã®ç”Ÿæˆï¼ˆçµ±åˆï¼‰---
            logger.info("âœï¸ Step 7: Generating final answer from cleaned context...")
            if self.final_answer_chain is None:
                logger.warning(
                    "âš ï¸ Final answer chain is not initialized. Returning cleaned context as answer."
                )
                answer = cleaned_context
            else:
                answer = await self.final_answer_chain.ainvoke(
                    {"cleaned_context": cleaned_context, "question": query}
                )

            # å›ç­”ã«å‚ç…§å…ƒURLã‚’è¿½è¨˜
            unique_sources = list(
                {
                    doc.metadata.get("source")
                    for doc in final_context_docs
                    if doc is not None
                }
            )

            logger.info("ğŸ“¬ Successfully generated an answer.")
            return {"answer": answer, "source_documents": unique_sources}

        except Exception as e:
            logger.exception(
                "ğŸ”¥ Error during Advanced RAG invocation for query: '%s'", query
            )
            return {"error": f"Failed to get an answer: {str(e)}"}

    def _rag_fusion(
        self, retrieved_results: List[List[Document]], k: int = 60
    ) -> List[Document]:
        """Reciprocal Rank Fusion (RRF)ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ï¼Œè¤‡æ•°ã®æ¤œç´¢çµæœã‚’çµ±åˆãƒ»ãƒ©ãƒ³ã‚¯ä»˜ã‘ã™ã‚‹ï¼"""
        fused_scores = {}
        doc_map = {}
        # å„æ¤œç´¢çµæœãƒªã‚¹ãƒˆã‚’ãƒ«ãƒ¼ãƒ—
        for docs in retrieved_results:
            # å„æ–‡æ›¸ã®é †ä½(rank)ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            for rank, doc in enumerate(docs):
                # æ–‡æ›¸ã‚’ä¸€æ„ã«è­˜åˆ¥ã™ã‚‹ãŸã‚ã®IDã‚’ç”Ÿæˆ
                doc_id = f"{doc.metadata.get('source', '')}_{doc.page_content[:100]}"
                doc_map[doc_id] = doc
                if doc_id not in fused_scores:
                    fused_scores[doc_id] = 0
                # RRFã®ã‚¹ã‚³ã‚¢è¨ˆç®—å¼: 1 / (rank + k)ï¼kã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å½±éŸ¿åº¦ã‚’èª¿æ•´ã™ã‚‹å®šæ•°ï¼
                fused_scores[doc_id] += 1 / (rank + k)

        # ç·åˆã‚¹ã‚³ã‚¢ã§é™é †ã«ã‚½ãƒ¼ãƒˆ
        reranked_results = sorted(
            fused_scores.items(), key=lambda x: x[1], reverse=True
        )
        # ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸIDã®ãƒªã‚¹ãƒˆã‹ã‚‰ï¼Œå…ƒã®Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’å†æ§‹ç¯‰ã—ã¦è¿”ã™
        return [doc_map[doc_id] for doc_id, score in reranked_results]


# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
rag_handler_instance = RAGHandler()
