import os
import jq
import logging
import hashlib
import asyncio
import json
from typing import Any, Dict, List, Generator
from tqdm import tqdm
from langchain_core.documents import Document
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# LangChain Caching Imports
from langchain_community.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore

from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import JSONLoader
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from chromadb.config import Settings

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s",
)
logger = logging.getLogger(__name__)


# --- å®šæ•°å®šç¾© ---
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "mxbai-embed-large")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3:latest")
RERANKER_MODEL = "BAAI/bge-reranker-base"
CHROMA_DB_PATH = "./db"
DATA_PATH = "./data/scraped_data.json"
CACHE_PATH = "./cache"
PARENT_DOCS_PATH = "./data/parent_docs.json"
PARENT_CHUNK_SIZE = 1024
CHILD_CHUNK_SIZE = 256
CHUNK_OVERLAP = 64
BATCH_SIZE = 128

logger.info("ðŸ§  Activating in-memory LLM cache...")
set_llm_cache(InMemoryCache())

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ---
query_expansion_template = """
### æŒ‡ç¤º
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åˆ†æžã—ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®å¤šæ§˜ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ã‚’åŸºã«ã€ç•°ãªã‚‹è¦–ç‚¹ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”¨ã„ãŸæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’3ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

### åŽ³æ ¼ãªãƒ«ãƒ¼ãƒ«
- **å‡ºåŠ›ã¯"queries"ã¨ã„ã†ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿**ã¨ã—ã€å‰å¾Œã«èª¬æ˜Žã‚„æŒ¨æ‹¶ãªã©ã®ä½™è¨ˆãªãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
- å¿…ãš3ã¤ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã€ã™ã¹ã¦æ—¥æœ¬èªžã§æ›¸ã„ã¦ãã ã•ã„ã€‚
- å‡ºåŠ›å½¢å¼ã¯å¿…ãš `{{"queries": ["ã‚¯ã‚¨ãƒª1", "ã‚¯ã‚¨ãƒª2", "ã‚¯ã‚¨ãƒª3"]}}` ã®ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

### å…ƒã®è³ªå•
{question}
"""

answer_generation_template = """
### æŒ‡ç¤º
ã‚ãªãŸã¯æ±äº¬å¤§å­¦ã®æƒ…å ±ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹å­¦ç”Ÿã‚„æ•™è·å“¡ã‹ã‚‰ã®è³ªå•ã«å›žç­”ã™ã‚‹ã€éžå¸¸ã«å„ªç§€ã§è¦ªåˆ‡ãªã‚µãƒãƒ¼ãƒˆæ‹…å½“è€…ã§ã™ã€‚
æä¾›ã•ã‚ŒãŸã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã‚’æ³¨æ„æ·±ãèª­ã¿è¾¼ã¿ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å›žç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

### ãƒ«ãƒ¼ãƒ«
1.  **è¨€èªž**: å›žç­”ã¯ã™ã¹ã¦æ—¥æœ¬èªžã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
2.  **å½¹å‰²ã®éµå®ˆ**: å¿…ãšã‚µãƒãƒ¼ãƒˆæ‹…å½“è€…ã¨ã—ã¦ã€ä¸å¯§ã§åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰é£ã„ã‚’ã—ã¦ãã ã•ã„ã€‚
3.  **æƒ…å ±ã®åŽ³å®ˆ**: å›žç­”ã¯ã€**æä¾›ã•ã‚ŒãŸã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿**ã‚’æ ¹æ‹ ã¨ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨˜è¼‰ã®ãªã„æƒ…å ±ã‚„ã€ã‚ãªãŸè‡ªèº«ã®çŸ¥è­˜ã‚’æ±ºã—ã¦è£œã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
4.  **æ®µéšŽçš„ãªèª¬æ˜Ž**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€å…·ä½“çš„ãªæ‰‹é †ã‚’æ®µéšŽçš„ã«èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚
5.  **çµ±åˆã¨è¦ç´„**: è¤‡æ•°ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚ç…§ã—ã€ãã‚Œã‚‰ã‚’è‡ªç„¶ãªæ–‡ç« ã«çµ±åˆãƒ»è¦ç´„ã—ã¦å›žç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æƒ…å ±ã®æ–­ç‰‡ã‚’ã‚³ãƒ”ãƒ¼ãƒšãƒ¼ã‚¹ãƒˆã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
6.  **æƒ…å ±ãŒãªã„å ´åˆ**: è³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€æ›–æ˜§ãªå›žç­”ã¯ã›ãšã€ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã”è³ªå•ã«é–¢ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨æ˜Žç¢ºã«å›žç­”ã—ã¦ãã ã•ã„ã€‚

### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
{context}

### è³ªå•
{question}

### å›žç­”
"""


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def metadata_func(record: dict, metadata: dict) -> dict:
    metadata["source"] = record.get("url")
    return metadata


def batch_generator(
    data: List[Any], batch_size: int
) -> Generator[List[Any], None, None]:
    for i in range(0, len(data), batch_size):
        yield data[i : i + batch_size]


def create_sha256_encoder(namespace: str):
    def sha256_encoder(payload: str) -> str:
        namespaced_payload = namespace.encode() + b":" + payload.encode()
        return hashlib.sha256(namespaced_payload).hexdigest()

    return sha256_encoder


class RAGHandler:
    def __init__(self):
        self.vector_store = None
        self.parent_docstore = {}
        self.llm = None
        self.json_llm = None
        self.reranker = None
        self.query_expansion_chain = None
        self.final_answer_chain = None
        logger.info("ðŸš¦ Initializing Advanced RAGHandler...")
        self._initialize()

    def _initialize(self):
        os.makedirs(CACHE_PATH, exist_ok=True)
        fs_store = LocalFileStore(CACHE_PATH)

        logger.info("ðŸ”Œ Connecting to Ollama at %s", OLLAMA_BASE_URL)
        ollama_embedder = OllamaEmbeddings(
            model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL
        )
        key_encoder = create_sha256_encoder(namespace=EMBEDDING_MODEL)
        cached_embedder = CacheBackedEmbeddings.from_bytes_store(
            ollama_embedder, fs_store, key_encoder=key_encoder
        )

        self.llm = ChatOllama(
            model=LLM_MODEL, temperature=0.2, top_k=40, base_url=OLLAMA_BASE_URL
        )
        self.json_llm = ChatOllama(
            model=LLM_MODEL, format="json", temperature=0, base_url=OLLAMA_BASE_URL
        )

        logger.info("ðŸ” Initializing Re-ranker: %s", RERANKER_MODEL)
        self.reranker = HuggingFaceCrossEncoder(model_name=RERANKER_MODEL)

        chroma_settings = Settings(anonymized_telemetry=False)
        if os.path.exists(CHROMA_DB_PATH) and os.path.exists(PARENT_DOCS_PATH):
            logger.info("ðŸ’¾ Loading existing vector store and parent docstore...")
            self.vector_store = Chroma(
                persist_directory=CHROMA_DB_PATH,
                embedding_function=cached_embedder,
                client_settings=chroma_settings,
            )
            with open(PARENT_DOCS_PATH, "r", encoding="utf-8") as f:
                parent_docs_data = json.load(f)
                self.parent_docstore = {
                    int(k): Document(**v) for k, v in parent_docs_data.items()
                }
            logger.info("âœ… Vector store and parent docstore loaded successfully.")
        else:
            self._create_sentence_window_db(cached_embedder, chroma_settings)

        logger.info("ðŸ”— Initializing LCEL chains...")
        query_prompt = PromptTemplate(
            template=query_expansion_template, input_variables=["question"]
        )
        self.query_expansion_chain = query_prompt | self.json_llm | JsonOutputParser()

        answer_prompt = PromptTemplate(
            template=answer_generation_template, input_variables=["context", "question"]
        )
        self.final_answer_chain = (
            {
                "context": lambda x: "\n---\n".join(
                    doc.page_content for doc in x["documents"]
                ),
                "question": lambda x: x["question"],
            }
            | answer_prompt
            | self.llm
            | StrOutputParser()
        )
        logger.info("ðŸš€ Advanced RAG Handler is ready.")

    def _create_sentence_window_db(self, embedder, settings):
        logger.warning(
            "ðŸ•³ï¸ Vector store not found. Creating a new one with Sentence-Window strategy..."
        )
        if not os.path.exists(DATA_PATH):
            raise FileNotFoundError(f"'{DATA_PATH}' not found.")

        documents = JSONLoader(
            file_path=DATA_PATH,
            jq_schema=".[]",
            content_key="content",
            json_lines=False,
            metadata_func=metadata_func,
        ).load()
        logger.info("ðŸ“„ Loaded %d raw documents.", len(documents))

        parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=PARENT_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
        )
        child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHILD_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
        )

        parent_documents = parent_splitter.split_documents(documents)
        self.parent_docstore = {i: doc for i, doc in enumerate(parent_documents)}
        logger.info("ðŸ“‘ Split into %d parent chunks.", len(parent_documents))

        child_documents = []
        for i, p_doc in enumerate(parent_documents):
            _sub_docs = child_splitter.split_documents([p_doc])
            for _doc in _sub_docs:
                _doc.metadata["parent_id"] = i
            child_documents.extend(_sub_docs)
        logger.info("ðŸ“‘ Created %d child chunks for retrieval.", len(child_documents))

        logger.info(
            "â³ Creating vector store. This may take a while (first time only)..."
        )
        self.vector_store = Chroma(
            persist_directory=CHROMA_DB_PATH,
            embedding_function=embedder,
            client_settings=settings,
        )
        doc_batches = batch_generator(child_documents, BATCH_SIZE)
        for batch in tqdm(
            doc_batches,
            desc="ðŸ“¦ Embedding and adding child chunks",
            total=len(child_documents) // BATCH_SIZE
            + (1 if len(child_documents) % BATCH_SIZE > 0 else 0),
        ):
            self.vector_store.add_documents(batch)

        logger.info("ðŸ’¾ Saving parent docstore to %s", PARENT_DOCS_PATH)
        parent_docs_data = {k: v.dict() for k, v in self.parent_docstore.items()}
        with open(PARENT_DOCS_PATH, "w", encoding="utf-8") as f:
            json.dump(parent_docs_data, f, ensure_ascii=False, indent=4)

        logger.info("âœ… Vector store and parent docstore created.")

    async def ask(self, query: str) -> Dict[str, Any]:
        if not self.vector_store:
            return {"error": "Vector store not initialized."}

        try:
            logger.info("ðŸ§  Step 1: Expanding query...")
            if self.query_expansion_chain is None:
                raise RuntimeError("Query expansion chain is not initialized.")
            response_dict = await self.query_expansion_chain.ainvoke(
                {"question": query}
            )
            expanded_queries = response_dict.get("queries", [])
            all_queries = [query] + expanded_queries
            logger.info(f"âš¡ Expanded queries: {all_queries}")

            logger.info("ðŸ” Step 2: Retrieving documents in parallel...")
            retriever = self.vector_store.as_retriever(search_kwargs={"k": 15})
            tasks = [retriever.ainvoke(q) for q in all_queries]
            retrieved_results = await asyncio.gather(*tasks)

            logger.info("âœ¨ Step 3: Fusing results with RRF...")
            fused_docs = self._rag_fusion(retrieved_results)
            logger.info(f"ðŸ—³ï¸ Fused to {len(fused_docs)} unique documents.")

            logger.info("ðŸ§ Step 4: Re-ranking top documents...")
            rerank_pairs = [(query, doc.page_content) for doc in fused_docs[:20]]
            if self.reranker is None:
                raise RuntimeError("Reranker is not initialized.")

            scores = self.reranker.score(rerank_pairs)
            scored_docs = sorted(
                zip(fused_docs, scores), key=lambda x: x[1], reverse=True
            )

            final_child_docs = [doc for doc, score in scored_docs[:4]]

            logger.info("ðŸ“– Step 5: Fetching full context from parent docstore...")
            parent_ids = {doc.metadata["parent_id"] for doc in final_child_docs}
            final_context_docs = [
                self.parent_docstore.get(pid)
                for pid in parent_ids
                if self.parent_docstore.get(pid) is not None
            ]

            logger.info(
                f"ðŸ“š Using {len(final_context_docs)} parent documents for context."
            )

            logger.info("âœï¸ Step 6: Generating final answer...")
            if self.final_answer_chain is None:
                raise RuntimeError("Final answer chain is not initialized.")
            answer = await self.final_answer_chain.ainvoke(
                {"documents": final_context_docs, "question": query}
            )

            unique_sources = [
                src
                for src in {
                    doc.metadata.get("source")
                    for doc in final_context_docs
                    if doc is not None
                }
                if src is not None
            ]
            if unique_sources:
                answer += "\n\n---\n\n**å‚è€ƒã«ã—ãŸæƒ…å ±æº:**\n"
                for url in sorted(unique_sources):
                    if url:
                        answer += f"- [{url}]({url})\n"

            logger.info("ðŸ“¬ Successfully generated an answer.")
            return {"answer": answer, "source_documents": unique_sources}

        except Exception as e:
            logger.exception(
                "ðŸ”¥ Error during Advanced RAG invocation for query: '%s'", query
            )
            return {"error": f"Failed to get an answer: {str(e)}"}

    def _rag_fusion(
        self, retrieved_results: List[List[Document]], k: int = 60
    ) -> List[Document]:
        fused_scores = {}
        doc_map = {}
        for docs in retrieved_results:
            for rank, doc in enumerate(docs):
                doc_id = f"{doc.metadata.get('source', '')}_{doc.page_content[:100]}"
                doc_map[doc_id] = doc
                if doc_id not in fused_scores:
                    fused_scores[doc_id] = 0
                fused_scores[doc_id] += 1 / (rank + k)

        reranked_results = sorted(
            fused_scores.items(), key=lambda x: x[1], reverse=True
        )
        return [doc_map[doc_id] for doc_id, score in reranked_results]


rag_handler_instance = RAGHandler()
