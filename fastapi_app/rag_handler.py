import os
import jq
import logging
import hashlib
from typing import Any, Dict, List, Generator
from tqdm import tqdm
from langchain_community.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import JSONLoader
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.chains import RetrievalQA
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from chromadb.config import Settings

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s",
)
logger = logging.getLogger(__name__)


# --- å®šæ•°å®šç¾© ---
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "mxbai-embed-large")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3:latest")
RERANKER_MODEL = "BAAI/bge-reranker-base"
CHROMA_DB_PATH = "./db"
DATA_PATH = "./data/scraped_data.json"
CACHE_PATH = "./cache"
CHUNK_SIZE = 512
CHUNK_OVERLAP = 128
BATCH_SIZE = 100

logger.info("ğŸ§  Activating in-memory LLM cache...")
set_llm_cache(InMemoryCache())

prompt_template = """
### æŒ‡ç¤º
ã‚ãªãŸã¯æ±äº¬å¤§å­¦ã®æƒ…å ±ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã™ã‚‹å­¦ç”Ÿã‚„æ•™è·å“¡ã‹ã‚‰ã®è³ªå•ã«å›ç­”ã™ã‚‹ï¼Œéå¸¸ã«å„ªç§€ã§è¦ªåˆ‡ãªã‚µãƒãƒ¼ãƒˆæ‹…å½“è€…ã§ã™ï¼
æä¾›ã•ã‚ŒãŸã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã‚’æ³¨æ„æ·±ãèª­ã¿è¾¼ã¿ï¼Œä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼

### ãƒ«ãƒ¼ãƒ«
1.  **è¨€èª**: å›ç­”ã¯ã™ã¹ã¦æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ï¼
2.  **å½¹å‰²ã®éµå®ˆ**: å¿…ãšã‚µãƒãƒ¼ãƒˆæ‹…å½“è€…ã¨ã—ã¦ï¼Œä¸å¯§ã§åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰é£ã„ã‚’ã—ã¦ãã ã•ã„ï¼
3.  **æƒ…å ±ã®å³å®ˆ**: å›ç­”ã¯ï¼Œ**æä¾›ã•ã‚ŒãŸã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿**ã‚’æ ¹æ‹ ã¨ã—ã¦ãã ã•ã„ï¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨˜è¼‰ã®ãªã„æƒ…å ±ã‚„ï¼Œã‚ãªãŸè‡ªèº«ã®çŸ¥è­˜ã‚’æ±ºã—ã¦è£œã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ï¼
4.  **æ®µéšçš„ãªèª¬æ˜**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ï¼Œå…·ä½“çš„ãªæ‰‹é †ã‚’æ®µéšçš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ï¼
5.  **çµ±åˆã¨è¦ç´„**: è¤‡æ•°ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚ç…§ã—ï¼Œãã‚Œã‚‰ã‚’è‡ªç„¶ãªæ–‡ç« ã«çµ±åˆãƒ»è¦ç´„ã—ã¦å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼æƒ…å ±ã®æ–­ç‰‡ã‚’ã‚³ãƒ”ãƒ¼ãƒšãƒ¼ã‚¹ãƒˆã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ï¼
6.  **æƒ…å ±ãŒãªã„å ´åˆ**: è³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ï¼Œæ›–æ˜§ãªå›ç­”ã¯ã›ãšï¼Œã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒï¼Œã”è³ªå•ã«é–¢ã™ã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ã€ã¨æ˜ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ï¼

### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
{context}

### è³ªå•
{question}

### å›ç­”
"""


def metadata_func(record: dict, metadata: dict) -> dict:
    metadata["source"] = record.get("url")
    return metadata


def batch_generator(
    data: List[Any], batch_size: int
) -> Generator[List[Any], None, None]:
    for i in range(0, len(data), batch_size):
        yield data[i : i + batch_size]


def create_sha256_encoder(namespace: str):
    """æŒ‡å®šã•ã‚ŒãŸåå‰ç©ºé–“ï¼ˆãƒ¢ãƒ‡ãƒ«åï¼‰ã§ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä»˜ã‘ãŸSHA-256ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹"""

    def sha256_encoder(payload: str) -> str:
        # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã®å‰ã«åå‰ç©ºé–“ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ï¼Œãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒƒã‚·ãƒ¥ã‚’ä¿è¨¼
        namespaced_payload = namespace.encode() + b":" + payload.encode()
        return hashlib.sha256(namespaced_payload).hexdigest()

    return sha256_encoder


class RAGHandler:
    def __init__(self):
        self.vector_store = None
        self.qa_chain = None
        logger.info("ğŸš¦ Initializing RAGHandler...")
        self._initialize()

    def _initialize(self):
        fs_store = LocalFileStore(CACHE_PATH)

        logger.info("ğŸ”Œ Connecting to Ollama at %s", OLLAMA_BASE_URL)
        ollama_embedder = OllamaEmbeddings(
            model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL
        )

        key_encoder = create_sha256_encoder(namespace=EMBEDDING_MODEL)
        cached_embedder = CacheBackedEmbeddings.from_bytes_store(
            ollama_embedder,
            fs_store,
            key_encoder=key_encoder,
        )

        chroma_settings = Settings(anonymized_telemetry=False)

        if os.path.exists(CHROMA_DB_PATH) and os.listdir(CHROMA_DB_PATH):
            logger.info("ğŸ’¾ Loading existing vector store from %s...", CHROMA_DB_PATH)
            self.vector_store = Chroma(
                persist_directory=CHROMA_DB_PATH,
                embedding_function=cached_embedder,
                client_settings=chroma_settings,
            )
            logger.info("âœ… Vector store loaded successfully.")
        else:
            logger.warning("ğŸ•³ï¸ Vector store not found. Creating a new one...")
            if not os.path.exists(DATA_PATH):
                logger.error(
                    "ğŸ”¥ '%s' not found. Please run scraper.py first.", DATA_PATH
                )
                raise FileNotFoundError(
                    f"'{DATA_PATH}' not found. Please run scraper.py first."
                )

            logger.info("ğŸ“‚ Loading documents from '%s'...", DATA_PATH)
            loader = JSONLoader(
                file_path=DATA_PATH,
                jq_schema=".[]",
                content_key="content",
                json_lines=False,
                metadata_func=metadata_func,
            )
            documents = loader.load()
            logger.info("ğŸ“„ Loaded %d documents.", len(documents))

            logger.info(
                "âœ‚ï¸ Splitting documents into chunks (size: %d, overlap: %d)...",
                CHUNK_SIZE,
                CHUNK_OVERLAP,
            )
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
            )
            docs = text_splitter.split_documents(documents)
            logger.info("ğŸ“‘ Split into %d chunks.", len(docs))

            logger.info(
                "â³ Creating vector store. This may take a while (first time only)..."
            )
            self.vector_store = Chroma(
                persist_directory=CHROMA_DB_PATH,
                embedding_function=cached_embedder,
                client_settings=chroma_settings,
            )

            doc_batches = batch_generator(docs, BATCH_SIZE)

            for batch in tqdm(
                doc_batches,
                desc="ğŸ“¦ Embedding and adding chunks",
                total=len(docs) // BATCH_SIZE
                + (1 if len(docs) % BATCH_SIZE > 0 else 0),
            ):
                self.vector_store.add_documents(batch)

            logger.info("âœ… Vector store created and persisted at %s", CHROMA_DB_PATH)

        llm = ChatOllama(
            model=LLM_MODEL, temperature=0.2, top_k=40, base_url=OLLAMA_BASE_URL
        )

        QA_CHAIN_PROMPT = PromptTemplate(
            input_variables=["context", "question"], template=prompt_template
        )

        logger.info("ğŸ” Initializing Re-ranker: %s", RERANKER_MODEL)
        base_retriever = self.vector_store.as_retriever(
            search_type="mmr", search_kwargs={"k": 10}
        )
        model = HuggingFaceCrossEncoder(model_name=RERANKER_MODEL)
        reranker = CrossEncoderReranker(model=model, top_n=4)
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=reranker, base_retriever=base_retriever
        )

        logger.info("ğŸ”— Initializing QA chain...")
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=compression_retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
        )
        logger.info("ğŸš€ RAG Handler is ready.")

    def ask(self, query: str) -> Dict[str, Any]:
        if self.qa_chain is None:
            logger.error("âŒ QA chain is not initialized. Cannot process query.")
            return {"error": "QA chain is not initialized."}

        try:
            logger.info("ğŸ“¨ Received query: %s", query)
            response = self.qa_chain.invoke({"query": query})

            answer = response.get("result", "").strip()
            source_docs = response.get("source_documents", [])

            logger.info("ğŸ“š Retrieved %d documents after re-ranking.", len(source_docs))

            unique_sources = []
            if source_docs:
                seen_urls = set()
                for i, doc in enumerate(source_docs):
                    source_url = doc.metadata.get("source")
                    logger.info("  - Source %d: %s", i + 1, source_url)
                    if source_url and source_url not in seen_urls:
                        seen_urls.add(source_url)
                        unique_sources.append(source_url)

                if unique_sources:
                    answer += "\n\n---\n\n**å‚è€ƒã«ã—ãŸæƒ…å ±æº:**\n"
                    for url in unique_sources:
                        answer += f"- [{url}]({url})\n"

            logger.info("ğŸ“¬ Successfully generated an answer.")
            return {
                "answer": answer,
                "source_documents": unique_sources,
            }
        except Exception as e:
            logger.exception(
                "ğŸ”¥ Error during QA chain invocation for query: '%s'", query
            )
            return {"error": f"Failed to get an answer from the QA chain: {str(e)}"}


rag_handler_instance = RAGHandler()
